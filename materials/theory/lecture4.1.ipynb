{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAe6vtCQCyVx"
      },
      "source": [
        "# Лекция: **Квантизация нейронных сетей**\n",
        "\n",
        "## **Введение:**\n",
        "Квантизация — ключевой метод оптимизации нейросетей для работы на устройствах с ограниченными ресурсами. Она позволяет уменьшить размер моделей, ускорить вычисления и снизить энергопотребление. В лекции разберём теоретические основы и практические аспекты применения квантизации.\n",
        "\n",
        "## **Цель работы:**\n",
        "Понять принципы квантизации, научиться применять её к весам и активациям моделей, оценить компромиссы между точностью и производительностью.\n",
        "\n",
        "## **Теоретическая часть**\n",
        "### 1. Основные понятия\n",
        "**Квантизация** — преобразование данных из формата с высокой точностью (float32) в компактный (int8). Аналогична дискретизации аналогового сигнала.\n",
        "\n",
        "**Преимущества:**\n",
        "- Уменьшение размера модели в 4 раза (float32 → int8);\n",
        "- Ускорение вычислений за счёт оптимизированных операций с целыми числами;\n",
        "- Экономия энергии для мобильных устройств.\n",
        "\n",
        "**Типы квантизации:**\n",
        "- **Аффинная (несимметричная):**\n",
        "  - Формулы:\n",
        "    - Масштаб: $S = \\frac{r_{\\text{max}} - r_{\\text{min}}}{q_{\\text{max}} - q_{\\text{min}}}$\n",
        "    - Zero-Point: $Z = \\left\\lfloor q_{\\text{min}} - \\frac{r_{\\text{min}}}{S} \\right\\rceil$\n",
        "  - Подходит для асимметричных данных (например, выход ReLU).\n",
        "\n",
        "- **Симметричная:**\n",
        "  - Формула масштаба: $S = \\frac{|r|_{\\text{max}}}{2^{N-1} - 1}$\n",
        "  - Нулевая точка всегда равна 0.\n",
        "\n",
        "### 2. Гранулярность\n",
        "Уровень детализации квантизации:\n",
        "- **По тензору:** одна константа масштаба на весь тензор.\n",
        "- **По строкам/столбцам:** отдельные константы для строк/столбцов (повышает точность).\n",
        "- **По блокам:** например, блоки 64x64 элементов (оптимально для борьбы с выбросами).\n",
        "\n",
        "### 3. Типы данных\n",
        "- **INT8:** 8-битные целые (−128 до 127). Основной формат для инференса.\n",
        "- **BFLOAT16:** 16-битные числа с экспонентой float32. Используется в обучении.\n",
        "- **NF4:** 4-битный формат для квантизации нормально распределённых весов.\n",
        "\n",
        "### 4. Проблема выбросов\n",
        "Выбросы в активациях (значения >> средних) нарушают точность квантизации. Решения:\n",
        "- **LLM.Int8:** Разделение матриц на части с выбросами и без.\n",
        "- **SmoothQuant:** Перенос выбросов из активаций в веса.\n",
        "---\n",
        "### 5. Математические основы\n",
        "#### 5.1 Формализация задачи\n",
        "Для тензора весов $W \\in \\mathbb{R}^{n \\times m}$ найти квантизованное представление $\\hat{W} \\in \\mathbb{Z}^{n \\times m}$ и масштаб $S \\in \\mathbb{R}$, чтобы:\n",
        "$$\n",
        "\\min_{S, \\hat{W}} ||W - S \\cdot \\hat{W}||_F^2\n",
        "$$\n",
        "где $||\\cdot||_F$ — норма Фробениуса.\n",
        "\n",
        "#### 5.2 Кривые искажения\n",
        "Сравнение ошибок при разных методах квантизации:\n",
        "```\n",
        "        равномерная       │       нелинейная\n",
        "    ●●●●●●●●●●●●●●        │    ▲\n",
        "    ●              ●       │    ●●●●\n",
        "    ●              ●       │   ●    ●\n",
        "━━━━●━━━━━━━━━━━━━━●━━━━   ━━━━●────●━━━\n",
        "```\n",
        "Нелинейные методы лучше сохраняют важные диапазоны значений.\n",
        "\n",
        "### 6. Уровни гранулярности\n",
        "#### 6.1 Анализ эффективности\n",
        "| Гранулярность   | Точность (TOP-1) | Скорость (FPS) |\n",
        "|-----------------|------------------|----------------|\n",
        "| Тензорная       | 72.3%            | 145            |\n",
        "| По каналам      | 75.1%            | 132            |\n",
        "| Блочная 64x64   | 76.8%            | 121            |\n",
        "\n",
        "#### 6.2 Практический пример\n",
        "Для матрицы активаций $A \\in \\mathbb{R}^{3x3}$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.2 & 25.1 & 3.4 \\\\\n",
        "0.5 & 18.6 & 2.9 \\\\\n",
        "1.8 & 22.3 & 4.1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "При блочной квантизации 2x2:\n",
        "1. Разбиваем матрицу на блоки\n",
        "2. Вычисляем масштаб для каждого блока\n",
        "3. Кодируем отдельно\n",
        "\n",
        "### 7. Аппаратная поддержка\n",
        "#### 7.1 Сравнение архитектур\n",
        "- **CPU:** AVX-512 VNNI (INT8)\n",
        "- **GPU:** Tensor Cores (FP8)\n",
        "- **NPU:** Аппаратные акселераторы для 4-битных операций\n",
        "\n",
        "#### 7.2 Энергоэффективность\n",
        "$$\n",
        "E = \\frac{\\text{TOPS}}{\\text{Вт}} \\times \\text{Битовая эффективность}\n",
        "$$\n",
        "Где битовая эффективность вычисляется как отношение точности квантизованной модели к исходной.\n",
        "\n",
        "### 8. Продвинутые методы\n",
        "#### 8.1 Адаптивная квантизация\n",
        "Динамическая регулировка битности в зависимости от важности слоёв:\n",
        "```python\n",
        "def adaptive_quant(layer):\n",
        "    sensitivity = gradient_norm(layer)\n",
        "    bits = 8 - 4 * sigmoid(sensitivity)\n",
        "    return quantize(layer, bits=round(bits))\n",
        "```\n",
        "\n",
        "#### 8.2 Гибридные подходы\n",
        "Комбинация разных типов данных в одной модели:\n",
        "- Веса: 4-бит NF4\n",
        "- Активации: 8-бит FP8\n",
        "- Градиенты: BF16\n",
        "\n",
        "### 9. Метрики качества\n",
        "Для оценки эффективности квантизации используют:\n",
        "1. **PSNR** (пиковое отношение сигнал/шум):\n",
        "$$\n",
        "\\text{PSNR} = 20 \\log_{10}\\left(\\frac{\\text{MAX}_I}{\\sqrt{\\text{MSE}}}\\right)\n",
        "$$\n",
        "2. **EMD** (Earth Mover's Distance) для распределений\n",
        "3. **Perplexity** для языковых моделей\n",
        "\n",
        "### 10. Кейс-стади: MobileNet v3\n",
        "| Версия       | Точность | Размер  | Энергопотребление |\n",
        "|--------------|----------|---------|--------------------|\n",
        "| FP32         | 75.2%    | 12 MB   | 3.2 Дж/кадр        |\n",
        "| INT8         | 74.8%    | 3 MB    | 0.9 Дж/кадр        |\n",
        "| Смешанная    | 75.1%    | 4.5 MB  | 1.1 Дж/кадр        |\n",
        "\n",
        "### 11. Практические применения квантизации в индустрии\n",
        "**Кейс 1: Голосовые помощники**  \n",
        "Модель распознавания ключевого слова \"Алиса\" в Яндекс.Станции:\n",
        "- Исходный размер: 15 МБ (FP32)\n",
        "- После квантизации: 4 МБ (INT8)\n",
        "- Задержка снижена с 120 мс до 35 мс\n",
        "\n",
        "**Кейс 2: Автономные автомобили**  \n",
        "Сеть семантической сегментации NVIDIA DRIVE:\n",
        "```python\n",
        "# Псевдокод оптимизации\n",
        "model = load_model('segnet.fp32')\n",
        "quant_config = {\n",
        "    'conv1': {'bits': 8, 'granularity': 'per_channel'},\n",
        "    'conv5': {'bits': 4, 'method': 'NF4'}\n",
        "}\n",
        "quantize_model(model, quant_config)\n",
        "```\n",
        "\n",
        "### 12. Сравнение алгоритмов квантизации\n",
        "| Метод          | Битность | Точность | Поддержка HW | Сложность |\n",
        "|----------------|----------|----------|--------------|-----------|\n",
        "| Post-Training  | 8-bit    | ███▌     | Все CPU      | Низкая    |\n",
        "| QAT            | 4-8 bit  | ████▋    | GPU/TPU      | Средняя   |\n",
        "| GPTQ           | 3-4 bit  | ██▌      | GPU          | Высокая   |\n",
        "| SmoothQuant    | 8-bit    | ████▊    | NVIDIA       | Средняя   |\n",
        "\n",
        "### 13. Влияние на архитектуры нейросетей\n",
        "**Сверточные сети (CNN):**  \n",
        "- Чувствительны к квантизации первых слоев\n",
        "- Решение: гибридная квантизация\n",
        "$$\n",
        "\\mathcal{L}_{quant} = \\alpha||W - \\hat{W}||_2 + \\beta\\mathcal{L}_{task}\n",
        "$$\n",
        "\n",
        "**Рекуррентные сети (RNN):**  \n",
        "Проблема накопления ошибки в скрытых состояниях:\n",
        "```\n",
        "Ошибка шага t+1 = f(Ошибка шага t, ε_quant)\n",
        "```\n",
        "\n",
        "### 14. Реализация в популярных фреймворках\n",
        "**PyTorch Dynamic Quantization:**\n",
        "```python\n",
        "import torch.quantization\n",
        "\n",
        "model = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "torch.save(quantized_model.state_dict(), 'quantized.pth')\n",
        "```\n",
        "\n",
        "**TensorFlow Lite:**\n",
        "```python\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "```\n",
        "\n",
        "### 15. Будущие тенденции\n",
        "1. **Нейроморфные вычисления:** Аналоговая квантизация в мемристорах\n",
        "2. **Квантовое квантование:** Использование кубитов для сжатия моделей\n",
        "3. **Биологически вдохновленные методы:** Имитация синаптической пластичности\n",
        "\n",
        "## **Заключение**\n",
        "К 2026 году 80% edge-устройств будут использовать квантизованные модели. Ключевые направления:\n",
        "- Автоматический выбор битности (AutoQ)\n",
        "- Аппаратно-программная коктейльная оптимизация\n",
        "- Квантизация с гарантированной точностью\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "A[FP32 Модель] --> B{Выбор метода}\n",
        "B -->|Мобильные| C[Post-Training]\n",
        "B -->|Серверные| D[GPTQ]\n",
        "B -->|Гибридные| E[SmoothQuant]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPll1ez-CyV0"
      },
      "source": [
        "## **Практическая часть**\n",
        "### Пример 1: Квантизация весов в PyTorch\n",
        "Квантование предобученной модели ResNet-18."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzIVKM3NCyV0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Загрузка модели\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Настройка квантизации\n",
        "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "# Подготовка и преобразование модели\n",
        "quantized_model = torch.quantization.prepare(model, inplace=False)\n",
        "quantized_model = torch.quantization.convert(quantized_model)\n",
        "\n",
        "# Проверка типа весов первого слоя\n",
        "print(quantized_model.conv1.weight().dtype)  # Вывод: torch.qint8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Y-8OMaCyV1"
      },
      "source": [
        "**Пояснение:**\n",
        "- `qconfig` задаёт конфигурацию квантизации для CPU.\n",
        "- `prepare` встраивает наблюдатели для сбора статистики активаций.\n",
        "- `convert` заменяет слои на квантизованные версии."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOpDKiY_CyV1"
      },
      "source": [
        "### Пример 2: Quantize-Aware Training в TensorFlow\n",
        "Эмуляция квантизации во время обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WvG7AOKCyV1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow_model_optimization.quantization.keras import quantize_model\n",
        "\n",
        "# Создание модели\n",
        "model = tf.keras.Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(784,)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Обёртка для Quantize-Aware Training\n",
        "qat_model = quantize_model(model)\n",
        "qat_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Обучение (пример с MNIST)\n",
        "# qat_model.fit(x_train, y_train, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUUyd82aCyV2"
      },
      "source": [
        "**Пояснение:**\n",
        "- `quantize_model` добавляет псевдоквантизационные узлы в слои.\n",
        "- Во время обратного прохода градиенты вычисляются для исходных весов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7LJHTTECyV2"
      },
      "source": [
        "### Пример 3: Реализация SmoothQuant\n",
        "Масштабирование активаций и весов для борьбы с выбросами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzUsJIpMCyV2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def smooth_quant(activations, weights, alpha=0.5):\n",
        "    # Вычисление масштабов\n",
        "    S_act = np.max(np.abs(activations), axis=0)\n",
        "    S_weights = np.max(np.abs(weights), axis=1)\n",
        "    S = (S_act ** alpha) / (S_weights ** (1 - alpha))\n",
        "\n",
        "    # Масштабирование\n",
        "    activations_scaled = activations / S\n",
        "    weights_scaled = weights * S[:, np.newaxis]\n",
        "    return activations_scaled, weights_scaled\n",
        "\n",
        "# Пример данных\n",
        "activations = np.random.randn(100, 256)  # Активации\n",
        "weights = np.random.randn(256, 512)      # Веса слоя\n",
        "aq, wq = smooth_quant(activations, weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PROlgzbECyV2"
      },
      "source": [
        "**Пояснение:**\n",
        "- `alpha=0.5` балансирует масштабирование между активациями и весами.\n",
        "- После масштабирования выбросы в активациях уменьшаются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r7-my4GCyV2"
      },
      "source": [
        "## **Заключение**\n",
        "**Преимущества квантизации:**\n",
        "- Уменьшение размера модели до 4 раз.\n",
        "- Ускорение инференса на 2-4x.\n",
        "\n",
        "**Недостатки:**\n",
        "- Потеря точности (0.5-2% для INT8).\n",
        "- Зависимость от аппаратной поддержки (INT8 на GPU vs CPU).\n",
        "\n",
        "**Рекомендации:**\n",
        "- Для мобильных устройств: Post-Training Quantization.\n",
        "- Для LLM: Использовать GPTQ или SmoothQuant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyCj5m-mCyV3"
      },
      "source": [
        "- [Habr](https://habr.com/ru/companies/yandex/articles/800945/)\n",
        "- [Методы оптимизации нейронных сетей](https://habr.com/ru/articles/318970/)\n",
        "- [ServerFlow](https://serverflow.ru/blog/stati/kvantizatsiya-ii-chto-eto-takoe-i-dlya-chego-nuzhno/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}