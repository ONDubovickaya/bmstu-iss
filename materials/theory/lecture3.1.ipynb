{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex_f9qEH7K_W"
      },
      "source": [
        "# Анализ параметров нейронной сети для Bag-of-Words\n",
        "\n",
        "В данном материале разбираются слои и параметры нейросети на основе Bag-of-Words (BoW), а также рассматриваются методы их оптимизации.\n",
        "\n",
        "## Введение\n",
        "Модель BoW использует простые, но эффективные слои для обработки текстовых данных. Корректное понимание функций каждого компонента и методов подбора гиперпараметров критически важно для достижения высокой точности классификации.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWOQTrH47K_Z"
      },
      "source": [
        "## 1. Создание последовательной модели\n",
        "```python\n",
        "model_text_bow_softmax = Sequential()\n",
        "```\n",
        "- **Что это?**\n",
        "  - `Sequential()` — контейнер для линейной архитектуры нейросети.\n",
        "- **Зачем это нужно?**\n",
        "  - Для простых моделей с последовательной передачей данных. Для сложных архитектур используется функциональный API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A3D67Ed7K_Z"
      },
      "source": [
        "## 2. Первый полносвязный слой\n",
        "```python\n",
        "model.add(Dense(200, input_dim=MAX_WORDS_COUNT, activation='relu'))\n",
        "```\n",
        "- **Что это?**\n",
        "  - `Dense(200)` — слой с 200 нейронами.\n",
        "  - `input_dim` — размерность входных данных (размер словаря BoW).\n",
        "  - `activation='relu'` — функция активации ReLU.\n",
        "- **Зачем это нужно?**\n",
        "  - Обучает веса для входных признаков и формирует скрытое представление данных.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqhOttTn7K_a"
      },
      "source": [
        "## 3. Слой Dropout\n",
        "```python\n",
        "model.add(Dropout(0.25))\n",
        "```\n",
        "- **Что это?**\n",
        "  - Случайное отключение 25% нейронов на каждом шаге обучения.\n",
        "- **Зачем это нужно?**\n",
        "  - Предотвращает переобучение за счет уменьшения зависимости модели от отдельных нейронов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFfNduz7K_a"
      },
      "source": [
        "## 4. Слой BatchNormalization\n",
        "```python\n",
        "model.add(BatchNormalization())\n",
        "```\n",
        "- **Что это?**\n",
        "  - Нормализация активаций по мини-батчу.\n",
        "- **Зачем это нужно?**\n",
        "  - Ускоряет обучение, стабилизирует градиенты и снижает требуемую точность инициализации весов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScoXcN6A7K_a"
      },
      "source": [
        "## 5. Выходной слой\n",
        "```python\n",
        "model.add(Dense(ClassCount, activation='softmax'))\n",
        "```\n",
        "- **Что это?**\n",
        "  - `Dense(ClassCount)` — слой с количеством нейронов = числу классов.\n",
        "  - `activation='softmax'` — преобразует выходы в вероятности.\n",
        "- **Зачем это нужно?**\n",
        "  - Предоставляет вероятностную оценку классов для решения задачи классификации.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9b7ukSQ7K_a"
      },
      "source": [
        "## Дополнительные параметры\n",
        "1. **Оптимизаторы:**\n",
        "   - `adam` (адаптивный оптимизатор).\n",
        "   - `sgd` (стохастический градиентный спуск).\n",
        "2. **Функции потерь:**\n",
        "   - `categorical_crossentropy` (для многоклассовой классификации).\n",
        "   - `binary_crossentropy` (для бинарной).\n",
        "3. **Метрики:**\n",
        "   - `accuracy` (точность).\n",
        "4. **Регуляризация:**\n",
        "   - `kernel_regularizer=l2(0.01)` — L2-регуляризация.\n",
        "5. **Дополнительные слои:**\n",
        "   - `GaussianNoise` — добавляет шум к входным данным.\n",
        "   - `Activation()` — явное указание функции активации.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyMB3fgE7K_b"
      },
      "source": [
        "## Методы подбора параметров\n",
        "1. **Эмпирический выбор:**\n",
        "   - Начальные значения (например, `200 нейронов`, `dropout=0.25`).\n",
        "2. **Grid Search:**\n",
        "   - Перебор всех возможных комбинаций параметров.\n",
        "3. **Random Search:**\n",
        "   - Случайный выбор значений для ускорения оптимизации.\n",
        "4. **Автоматические инструменты:**\n",
        "   - **Keras Tuner**, **Optuna**, **Hyperopt**.\n",
        "5. **Кросс-валидация:**\n",
        "   - Оценка модели на нескольких подвыборках данных.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2YlK-e57K_b"
      },
      "source": [
        "## Пример автоматической оптимизации\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('units', min_value=100, max_value=300, step=50),\n",
        "        input_dim=MAX_WORDS_COUNT,\n",
        "        activation='relu'))\n",
        "    model.add(Dropout(\n",
        "        rate=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(ClassCount, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='text_classification'\n",
        ")\n",
        "tuner.search(X_train, y_train,\n",
        "            epochs=10,\n",
        "            validation_data=(X_val, y_val))\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTjg_IjA7K_b"
      },
      "source": [
        "**## Пример оптимизации для BoW-модели**\n",
        "### Цель:\n",
        "Подобрать параметры:\n",
        "1. `units` (количество нейронов в первом слое).\n",
        "2. `dropout` (процент отключения нейронов).\n",
        "\n",
        "### Используемый инструмент:\n",
        "- **Keras Tuner** — библиотека для автоматической оптимизации.\n",
        "- **RandomSearch** — алгоритм случайного перебора параметров.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6uMohCO7K_b"
      },
      "source": [
        "### Этап 1: Импорт библиотек\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from kerastuner.tuners import RandomSearch\n",
        "```\n",
        "- **Sequential**: Для создания последовательной модели.\n",
        "- **Dense/Dropout/BatchNormalization**: Слои нейронной сети.\n",
        "- **RandomSearch**: Класс для автоматического поиска параметров.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8zKY4fE7K_b"
      },
      "source": [
        "### Этап 2: Функция построения модели\n",
        "```python\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('units', min_value=100, max_value=300, step=50),\n",
        "        input_dim=MAX_WORDS_COUNT,\n",
        "        activation='relu'))\n",
        "    model.add(Dropout(\n",
        "        rate=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(ClassCount, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "```\n",
        "#### Ключевые моменты:\n",
        "1. **`hp.Int('units', ...)`**:\n",
        "   - Поиск значения от 100 до 300 с шагом 50.\n",
        "   - Примеры: 100, 150, 200, 250, 300.\n",
        "2. **`hp.Float('dropout', ...)`**:\n",
        "   - Поиск значения от 0.1 до 0.5 с шагом 0.1.\n",
        "   - Примеры: 0.1, 0.2, ..., 0.5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8SXZQzw7K_b"
      },
      "source": [
        "### Этап 3: Настройка тюнера\n",
        "```python\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='text_classification'\n",
        ")\n",
        "```\n",
        "#### Параметры:\n",
        "1. **`objective='val_accuracy'`**:\n",
        "   - Модель оптимизируется по точности на валидационных данных.\n",
        "2. **`max_trials=10`**:\n",
        "   - Количество уникальных комбинаций параметров для тестирования.\n",
        "3. **`executions_per_trial=2`**:\n",
        "   - Каждая комбинация обучается 2 раза для стабилизации результатов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7frLJ9Kn7K_b"
      },
      "source": [
        "### Этап 4: Запуск оптимизации\n",
        "```python\n",
        "tuner.search(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "```\n",
        "- **`search()`**: Запускает перебор параметров.\n",
        "- **`epochs=10`**: Каждая модель обучается 10 эпох.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfAcE_ot7K_b"
      },
      "source": [
        "### Этап 5: Получение лучшей модели\n",
        "```python\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "```\n",
        "- **`get_best_models()`**: Возвращает модель с наилучшей `val_accuracy`.\n",
        "- **`get_best_hyperparameters()`**: Возвращает оптимальные значения параметров.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-c3TYJC7K_c"
      },
      "source": [
        "## Почему именно эти параметры?\n",
        "1. **`units` (количество нейронов)**:\n",
        "   - **100–300**: Баланс между сложностью модели и риском переобучения.\n",
        "   - Меньше нейронов → проще модель, но возможен недообучение.\n",
        "   - Больше нейронов → более точная модель, но требует больше ресурсов.\n",
        "2. **`dropout` (процент отключения)**:\n",
        "   - **0.1–0.5**: Типичный диапазон для текстовых задач.\n",
        "   - Высокие значения (0.5) сильнее регуляризуют, но могут уменьшить качество.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkHTEaqu7K_c"
      },
      "source": [
        "## Улучшения и расширения\n",
        "1. **Добавить другие параметры**:\n",
        "   - `learning_rate` для оптимизатора `adam`:\n",
        "     ```python\n",
        "     optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', [0.001, 0.01]))\n",
        "     ```\n",
        "   - `batch_size`:\n",
        "     ```python\n",
        "     batch_size=hp.Int('batch_size', min_value=32, max_value=128, step=32)\n",
        "     ```\n",
        "2. **Использовать другие алгоритмы**:\n",
        "   - **BayesianOptimization**: Использует статистические модели для более умного выбора параметров.\n",
        "   - **Hyperband**: Раннее отсечение слабых моделей для экономии времени.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ctBzjkt7K_c"
      },
      "source": [
        "## Важные замечания\n",
        "1. **Время выполнения**:\n",
        "   - Чем больше `max_trials` и `epochs`, тем дольше оптимизация.\n",
        "   - Для больших моделей используйте `max_trials=5` и `epochs=5` для быстрого тестирования.\n",
        "2. **Ресурсы**:\n",
        "   - Требуется GPU/TPU для больших сетей.\n",
        "3. **Репродуктивность**:\n",
        "   - Используйте `random_state` для воспроизводимости результатов.\n",
        "4. **Интерпретация результатов**:\n",
        "   - Лучшие параметры сохраняются в `best_hyperparameters.values`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkTlJcbc7K_c"
      },
      "source": [
        "## Пример вывода\n",
        "После выполнения `tuner.search()` вы увидите:\n",
        "```\n",
        "Search: Running search...\n",
        "Epoch 1/10: loss: 0.6, val_accuracy: 0.82\n",
        "...\n",
        "Best val_accuracy: 0.85 (параметры: units=200, dropout=0.3)\n",
        "```\n",
        "- **Итог**: Модель с 200 нейронами и dropout=0.3 показала наилучшую точность.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g_RJQRq7K_c"
      },
      "source": [
        "## Итоги\n",
        "- **Слои:**\n",
        "  - `Dense` — основа для обучения.\n",
        "  - `Dropout`, `BatchNormalization` — регуляризация и стабилизация.\n",
        "  - `softmax` — решение задачи классификации.\n",
        "- **Подбор параметров:**\n",
        "  - Используйте комбинацию эмпирического выбора и автоматических инструментов (например, Keras Tuner).\n",
        "- **Ключевые принципы:**\n",
        "  - Валидация на тестовых данных.\n",
        "  - Баланс между сложностью модели и риском переобучения.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqJ2mQAR7K_c"
      },
      "source": [
        "## Контрольные вопросы\n",
        "1. Что такое гиперпараметры и зачем их оптимизируют?\n",
        "2. Какие параметры подбираются в примере? Для чего?\n",
        "3. В чем разница между `GridSearch` и `RandomSearch`?\n",
        "4. Какие параметры можно добавить в функцию `build_model` для улучшения модели?\n",
        "5. Почему важно использовать `executions_per_trial=2`?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glmlct0W7K_c"
      },
      "source": [
        "## Домашнее задание\n",
        "1. Реализуйте оптимизацию параметра `learning_rate` в примере.\n",
        "2. Сравните результаты `RandomSearch` и `Hyperband` для той же задачи.\n",
        "3. Объясните, почему dropout=0.5 может быть хуже dropout=0.2 для вашей модели.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8ScMAMA7K_c"
      },
      "source": [
        "**# Анализ параметров нейросети с использованием Embedding**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk1dVkeu7K_c"
      },
      "source": [
        "## Анализ модели\n",
        "Пример модели:\n",
        "```python\n",
        "model_text_emb_20 = Sequential()\n",
        "model_text_emb_20.add(Embedding(input_dim=MAX_WORDS_COUNT, output_dim=20, input_length=WIN_SIZE))\n",
        "model_text_emb_20.add(SpatialDropout1D(0.2))\n",
        "model_text_emb_20.add(Flatten())\n",
        "model_text_emb_20.add(BatchNormalization())\n",
        "model_text_emb_20.add(Dense(200, activation='relu'))\n",
        "model_text_emb_20.add(Dropout(0.2))\n",
        "model_text_emb_20.add(BatchNormalization())\n",
        "model_text_emb_20.add(Dense(ClassCount, activation='softmax'))\n",
        "```\n",
        "\n",
        "### Пошаговое описание слоев:\n",
        "1. **Embedding**:\n",
        "   - **`input_dim=MAX_WORDS_COUNT`**: Количество уникальных слов в словаре.\n",
        "   - **`output_dim=20`**: Размер векторного представления (эмбеддинга) для каждого слова.\n",
        "   - **`input_length=WIN_SIZE`**: Длина входного текста (например, максимальное количество слов в документе).\n",
        "   - **Назначение**: Преобразует текст в плотные векторы фиксированной длины.\n",
        "\n",
        "2. **SpatialDropout1D(0.2)**:\n",
        "   - **`rate=0.2`**: Отключает 20% единиц в каждом временном шаге (например, слове).\n",
        "   - **Назначение**: Регуляризация для предотвращения переобучения в последовательностях.\n",
        "\n",
        "3. **Flatten()**:\n",
        "   - **Назначение**: Преобразует 3D-выход Embedding в 2D-тензор для подачи в Dense-слои.\n",
        "\n",
        "4. **BatchNormalization()**:\n",
        "   - **Назначение**: Нормализует выходы предыдущего слоя, ускоряя обучение и стабилизируя градиенты.\n",
        "\n",
        "5. **Dense(200, activation='relu')**:\n",
        "   - **`units=200`**: Количество нейронов в скрытом слое.\n",
        "   - **`activation='relu'`**: Функция активации для нелинейности.\n",
        "   - **Назначение**: Обучает сложные комбинации признаков.\n",
        "\n",
        "6. **Dropout(0.2)**:\n",
        "   - **`rate=0.2`**: Отключает 20% нейронов на каждом шаге обучения.\n",
        "   - **Назначение**: Дополнительная регуляризация.\n",
        "\n",
        "7. **BatchNormalization()**:\n",
        "   - **Назначение**: Нормализация после Dropout для стабилизации обучения.\n",
        "\n",
        "8. **Dense(ClassCount, activation='softmax')**:\n",
        "   - **`units=ClassCount`**: Количество классов.\n",
        "   - **`activation='softmax'`**: Преобразует выходы в вероятности.\n",
        "   - **Назначение**: Финальное предсказание класса.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZA0iXId7K_c"
      },
      "source": [
        "## Дополнительные параметры\n",
        "1. **Оптимизаторы**:\n",
        "   - `optimizer='adam'` — адаптивный оптимизатор.\n",
        "   - `optimizer='sgd'` — стохастический градиентный спуск.\n",
        "2. **Функции потерь**:\n",
        "   - `loss='categorical_crossentropy'` — для многоклассовой классификации.\n",
        "   - `loss='sparse_categorical_crossentropy'` — если метки не закодированы в one-hot.\n",
        "3. **Метрики**:\n",
        "   - `metrics=['accuracy']` — оценка точности модели.\n",
        "4. **Регуляризация**:\n",
        "   - `kernel_regularizer=l2(0.01)` — L2-регуляризация для весов.\n",
        "5. **Дополнительные слои**:\n",
        "   - `LSTM`/`GRU` — для обработки последовательностей.\n",
        "   - `Conv1D` — для извлечения локальных паттернов.\n",
        "6. **Параметры Embedding**:\n",
        "   - `mask_zero=True` — игнорирование пустых значений.\n",
        "   - `embeddings_initializer='uniform'` — метод инициализации весов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "techQYki7K_c"
      },
      "source": [
        "## Методы подбора гиперпараметров\n",
        "### 1. Эмпирический выбор\n",
        "- **Пример**: Начальные значения:\n",
        "  - `output_dim=20` (размер эмбеддинга).\n",
        "  - `units=200` (количество нейронов).\n",
        "  - `dropout=0.2` (примерно 20% регуляризации).\n",
        "\n",
        "### 2. Grid Search\n",
        "- **Пример**: Перебор `output_dim` в диапазоне [10, 30] и `dropout` в [0.1, 0.3].\n",
        "\n",
        "### 3. Random Search\n",
        "- **Пример**: Случайный выбор значений:\n",
        "  ```python\n",
        "  hp.Int('output_dim', min_value=10, max_value=50, step=5)\n",
        "  hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "  ```\n",
        "\n",
        "### 4. Автоматизация с Keras Tuner\n",
        "- **Пример**: Оптимизация `output_dim`, `units`, `dropout`:\n",
        "  ```python\n",
        "  from kerastuner import RandomSearch\n",
        "\n",
        "  def build_model(hp):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(\n",
        "          input_dim=MAX_WORDS_COUNT,\n",
        "          output_dim=hp.Int('output_dim', 10, 50, 5),\n",
        "          input_length=WIN_SIZE))\n",
        "      model.add(SpatialDropout1D(hp.Float('spatial_dropout', 0.1, 0.5, 0.1)))\n",
        "      model.add(Flatten())\n",
        "      model.add(Dense(\n",
        "          units=hp.Int('dense_units', 100, 300, 50),\n",
        "          activation='relu'))\n",
        "      model.add(Dropout(hp.Float('dropout', 0.1, 0.5, 0.1)))\n",
        "      model.add(Dense(ClassCount, activation='softmax'))\n",
        "      model.compile(optimizer='adam',\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "      return model\n",
        "\n",
        "  tuner = RandomSearch(\n",
        "      build_model,\n",
        "      objective='val_accuracy',\n",
        "      max_trials=10,\n",
        "      executions_per_trial=2,\n",
        "      directory='tuning',\n",
        "      project_name='text_classification'\n",
        "  )\n",
        "  tuner.search(X_train, y_train,\n",
        "              epochs=5,\n",
        "              validation_data=(X_val, y_val))\n",
        "  ```\n",
        "  - **Результат**: Оптимизация по метрике `val_accuracy`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWd9PG2m7K_c"
      },
      "source": [
        "## Подробный разбор примера автоматизированной оптимизации с Keras Tuner\n",
        "\n",
        "Приведенный ниже пример демонстрирует, как автоматически подобрать оптимальные значения гиперпараметров для модели с **Embedding-слоем** с помощью **Keras Tuner**. Разберем каждый этап подробно.\n",
        "\n",
        "### Пример кода:\n",
        "```python\n",
        "from kerastuner import RandomSearch\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    # Слой Embedding с оптимизируемым output_dim\n",
        "    model.add(Embedding(\n",
        "        input_dim=MAX_WORDS_COUNT,\n",
        "        output_dim=hp.Int('output_dim', min_value=10, max_value=50, step=5),\n",
        "        input_length=WIN_SIZE\n",
        "    ))\n",
        "    # SpatialDropout1D с оптимизируемым процентом отключения\n",
        "    model.add(SpatialDropout1D(\n",
        "        hp.Float('spatial_dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "    ))\n",
        "    model.add(Flatten())\n",
        "    # Полносвязный слой с оптимизируемым количеством нейронов\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('dense_units', min_value=100, max_value=300, step=50),\n",
        "        activation='relu'\n",
        "    ))\n",
        "    model.add(Dropout(\n",
        "        hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "    ))\n",
        "    model.add(Dense(ClassCount, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Настройка тюнера\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='tuning',\n",
        "    project_name='text_classification'\n",
        ")\n",
        "\n",
        "# Запуск оптимизации\n",
        "tuner.search(\n",
        "    X_train, y_train,\n",
        "    epochs=5,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "```\n",
        "\n",
        "### Пошаговое объяснение:\n",
        "\n",
        "#### 1. Функция `build_model(hp)`\n",
        "- **`hp.Int('output_dim', min_value=10, max_value=50, step=5)`**:\n",
        "  - Ищет оптимальный размер векторного представления (эмбеддинга) для слов.\n",
        "  - Возможные значения: 10, 15, 20, ..., 50.\n",
        "  - **Почему?**:\n",
        "    - Маленькие значения (10–20) подходят для простых задач с небольшим объемом данных.\n",
        "    - Большие значения (50) могут улучшить качество, но требуют больше ресурсов.\n",
        "\n",
        "- **`hp.Float('spatial_dropout', min_value=0.1, max_value=0.5, step=0.1)`**:\n",
        "  - Оптимизирует процент отключения временных шагов (например, слов) в `SpatialDropout1D`.\n",
        "  - Возможные значения: 0.1, 0.2, ..., 0.5.\n",
        "  - **Почему?**:\n",
        "    - Низкие значения (0.1–0.3) сохраняют больше информации, но рискуют переобучиться.\n",
        "    - Высокие значения (0.5) сильнее регуляризуют, но могут уменьшить качество.\n",
        "\n",
        "- **`hp.Int('dense_units', min_value=100, max_value=300, step=50)`**:\n",
        "  - Ищет оптимальное количество нейронов в первом Dense-слое.\n",
        "  - Возможные значения: 100, 150, 200, 250, 300.\n",
        "  - **Почему?**:\n",
        "    - Больше нейронов → более сложная модель, но требует больше данных.\n",
        "    - Диапазон 100–300 — баланс между точностью и вычислительными ресурсами.\n",
        "\n",
        "- **`hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)`**:\n",
        "  - Оптимизирует процент отключения нейронов в Dense-слое.\n",
        "  - **Почему?**:\n",
        "    - Умеренные значения (0.2–0.3) обычно работают лучше для текстовых задач.\n",
        "\n",
        "#### 2. Настройка тюнера (`RandomSearch`)\n",
        "- **`objective='val_accuracy'`**:\n",
        "  - Цель: максимизировать точность на валидационных данных.\n",
        "- **`max_trials=10`**:\n",
        "  - Выполняется 10 экспериментов, в каждом из которых случайным образом выбираются параметры.\n",
        "- **`executions_per_trial=2`**:\n",
        "  - Каждая комбинация параметров обучается 2 раза для усреднения результатов.\n",
        "- **`directory='tuning'` и `project_name='text_classification'`**:\n",
        "  - Папка для сохранения результатов оптимизации.\n",
        "\n",
        "#### 3. Запуск оптимизации (`tuner.search()`)\n",
        "- **`epochs=5`**:\n",
        "  - Каждая модель обучается 5 эпох. Для экономии времени используется небольшое количество эпох на этапе поиска.\n",
        "- **`validation_data=(X_val, y_val)`**:\n",
        "  - Проверка качества модели на валидационных данных после каждой эпохи.\n",
        "\n",
        "#### 4. Результаты\n",
        "- После завершения, `tuner` выводит таблицу с лучшими комбинациями параметров и их метриками:\n",
        "  ```\n",
        "  Search: Running search...\n",
        "  Epoch 1/5: loss: 0.7, val_accuracy: 0.81\n",
        "  ...\n",
        "  Best val_accuracy: 0.85 (параметры: output_dim=30, spatial_dropout=0.2, dense_units=200, dropout=0.25)\n",
        "  ```\n",
        "- **Итог**: Лучшая модель использует:\n",
        "  - Размер эмбеддинга **30**.\n",
        "  - **20%** отключения в `SpatialDropout1D`.\n",
        "  - **200 нейронов** в Dense-слое.\n",
        "  - **25%** Dropout после Dense-слоя.\n",
        "\n",
        "### Почему именно эти параметры?\n",
        "1. **`output_dim=30`**:\n",
        "   - Средний размер эмбеддинга, который балансирует между информативностью и вычислительными затратами.\n",
        "2. **`spatial_dropout=0.2`**:\n",
        "   - Низкая регуляризация для текстовых данных, где важны все слова.\n",
        "3. **`dense_units=200`**:\n",
        "   - Среднее количество нейронов, позволяющее избежать переобучения и сохранить точность.\n",
        "4. **`dropout=0.25`**:\n",
        "   - Умеренная регуляризация для предотвращения переобучения в Dense-слое.\n",
        "\n",
        "### Улучшения и советы\n",
        "1. **Добавьте другие параметры**:\n",
        "   - `learning_rate` для оптимизатора:\n",
        "     ```python\n",
        "     optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('lr', [0.001, 0.0001]))\n",
        "     ```\n",
        "2. **Используйте другие алгоритмы**:\n",
        "   - **BayesianOptimization** для более умного выбора параметров.\n",
        "   - **Hyperband** для ранней остановки низкоэффективных моделей.\n",
        "3. **Проверьте стабильность**:\n",
        "   - Увеличьте `executions_per_trial=3` для более надежных результатов.\n",
        "\n",
        "### Ошибки и их исправление\n",
        "1. **Медленный поиск**:\n",
        "   - Уменьшите `max_trials` или `epochs` для экономии времени.\n",
        "2. **Переобучение**:\n",
        "   - Увеличьте `spatial_dropout` или `dropout`.\n",
        "3. **Низкая точность**:\n",
        "   - Увеличьте `output_dim` или `dense_units`.\n",
        "   - Добавьте дополнительные слои (например, `Conv1D`).\n",
        "\n",
        "### Важные замечания\n",
        "- **Время выполнения**: Для 10 экспериментов с 5 эпохами на каждом этапе может потребоваться несколько минут.\n",
        "- **Репродуктивность**: Используйте `random_state` для воспроизводимости результатов.\n",
        "- **Глубина модели**: Для сложных задач можно добавить дополнительные Dense-слои с оптимизацией их параметров.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URKXlija7K_c"
      },
      "source": [
        "## Пример разбора модели\n",
        "### 1. **Embedding**:\n",
        "- **`output_dim=20`**:\n",
        "  - Маленькие значения (10–50) часто используются для текстов небольшого объема.\n",
        "  - Большие значения (100–300) требуют больше данных, но могут улучшить качество.\n",
        "\n",
        "### 2. **SpatialDropout1D(0.2)**:\n",
        "- **Почему 0.2?**:\n",
        "  - Низкий уровень регуляризации для текстовых данных, где потеря части слов может быть полезной.\n",
        "  - Высокие значения (0.5+) могут привести к потере важной информации.\n",
        "\n",
        "### 3. **Flatten()**:\n",
        "- **Назначение**: Сжатие 3D-входа (batch_size, sequence_length, embedding_dim) в 2D (batch_size, features).\n",
        "- **Альтернатива**: Использование `GlobalAveragePooling1D` или `GlobalMaxPooling1D` для усреднения/максимизации по временной оси.\n",
        "\n",
        "### 4. **BatchNormalization()**:\n",
        "- **Почему дважды?**:\n",
        "  - После Flatten() — для стабилизации после SpatialDropout.\n",
        "  - После Dense() — для нормализации перед Dropout.\n",
        "\n",
        "### 5. **Dense(200, activation='relu')**:\n",
        "- **`units=200`**:\n",
        "  - Баланс между сложностью и вычислительными ресурсами.\n",
        "  - Можно попробовать 100–300 для разных задач.\n",
        "\n",
        "### 6. **Dropout(0.2)**:\n",
        "- **Почему 0.2?**:\n",
        "  - Умеренная регуляризация для предотвращения переобучения в Dense-слое.\n",
        "  - Для больших сетей можно увеличить до 0.3–0.5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyjGGpXq7K_c"
      },
      "source": [
        "## Контрольные вопросы\n",
        "1. Зачем используется слой `Embedding` в модели?\n",
        "2. В чем разница между `Dropout` и `SpatialDropout1D`?\n",
        "3. Какие параметры можно оптимизировать в слое `Embedding`?\n",
        "4. Почему применяется `Flatten()` после `Embedding`?\n",
        "5. Какие алгоритмы можно использовать для автоматизированной оптимизации гиперпараметров?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj22kYx07K_d"
      },
      "source": [
        "## Домашнее задание\n",
        "1. Измените параметр `output_dim` в Embedding и сравните точность модели.\n",
        "2. Добавьте в модель слой `Conv1D` и настройте его параметры.\n",
        "3. Реализуйте оптимизацию `output_dim` и `units` с помощью Keras Tuner.\n",
        "4. Объясните, почему `BatchNormalization` полезен перед активацией, а не после.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQOYF_-x7K_d"
      },
      "source": [
        "## **Продвинутые методы классификации текста**\n",
        "Список методов, превосходящих простые BoW и базовые embedding:\n",
        "\n",
        "| Метод                          | Описание                                                                |\n",
        "|-------------------------------|--------------------------------------------------------------------------|\n",
        "| **RNN (Recurrent Neural Networks)**       | Обрабатывают последовательности, сохраняя контекст           |\n",
        "| **LSTM/GRU**                   | Разновидности RNN, решающие проблему долгой зависимости                 |\n",
        "| **CNN (Convolutional Neural Networks)**   | Ищут локальные признаки в тексте через фильтры               |\n",
        "| **Transformers**               | Работают с самозависимыми attention-механизмами (например, BERT, GPT)   |\n",
        "| **BRNN (Bidirectional RNN)**   | Анализируют контекст в обоих направлениях                               |\n",
        "| **N-gram-based networks**       | Используют N-граммы без явного токенизации                             |\n",
        "| **Hybrid models**              | Комбинации embedding + CNN/RNN/Transformers для улучшения точности      |\n",
        "\n",
        "Каждый метод решает задачи контекста, семантики или масштабируемости.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-HtHThS7K_d"
      },
      "source": [
        "# Метод Transformers: подробное объяснение\n",
        "## Основы\n",
        "1. **Проблема предыдущих моделей**:\n",
        "   - RNN/GRU/LSTM сталкивались с проблемой долгой зависимости (long-term dependency) и медленной обработкой из-за последовательного характера [1](https://medium.com/towards-data-science/transformers-141e32e69591).\n",
        "   - CNN могли параллельно обрабатывать данные, но плохо учитывали глобальный контекст [6](https://medium.com/@punya8147_26846/differences-between-transformers-and-traditional-sequence-models-e53c7e1b849a).\n",
        "\n",
        "2. **Ключевая идея Transformers**:\n",
        "   - **Self-attention**: Механизм, который позволяет модели учитывать влияние всех слов в предложении на каждое конкретное слово [1](https://medium.com/towards-data-science/transformers-141e32e69591).\n",
        "   - **Параллельная обработка**: Вместо обработки последовательности пошагово, обрабатываются все токены одновременно [6](https://medium.com/@punya8147_26846/differences-between-transformers-and-traditional-sequence-models-e53c7e1b849a).\n",
        "\n",
        "3. **Основные компоненты**:\n",
        "   - **Encoder**: Преобразует входные токены в векторы, учитывая контекст [3](https://www.datacamp.com/tutorial/how-transformers-work),[4](https://medium.com/@nimritakoul01/the-transformer-model-all-components-simply-explained-series-38c76ecdeeef).\n",
        "   - **Decoder**: Используется для генерации выходных последовательностей (например, в машинном переводе) [4](https://medium.com/@nimritakoul01/the-transformer-model-all-components-simply-explained-series-38c76ecdeeef), [9](https://myscale.com/blog/key-components-transformer-models-understanding/).\n",
        "   - **Self-attention layer**: Вычисляет важность каждого токена относительно других [2](https://www.quora.com/What-are-the-key-components-of-the-Transformer-architecture-and-how-do-they-work-in-ChatGPT), [10](https://lih-verma.medium.com/components-of-transformer-architecture-748f74a1a40d).\n",
        "   - **Positional encoding**: Добавляет информацию о позиции токена, так как сам attention не учитывает порядок [9](https://myscale.com/blog/key-components-transformer-models-understanding/), [10](https://lih-verma.medium.com/components-of-transformer-architecture-748f74a1a40d).\n",
        "\n",
        "4. **Как работает self-attention**:\n",
        "   - Для каждого токена вычисляются три вектора: Query (запрос), Key (ключ), Value (значение).\n",
        "   - Важность токена определяется через скалярное произведение Query и Key других токенов.\n",
        "   - Итоговый вектор формируется как взвешенная сумма Value-векторов [2](https://www.quora.com/What-are-the-key-components-of-the-Transformer-architecture-and-how-do-they-work-in-ChatGPT), [10](https://lih-verma.medium.com/components-of-transformer-architecture-748f74a1a40d).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtDnnaE7K_d"
      },
      "source": [
        "## Общий синтаксис и архитектура\n",
        "### Структура модели:\n",
        "1. **Encoder**:\n",
        "   - Слой **self-attention**: Вычисляет важность токенов.\n",
        "   - Слой **feed-forward**: Обрабатывает векторы через полносвязные слои для извлечения признаков [2](https://www.quora.com/What-are-the-key-components-of-the-Transformer-architecture-and-how-do-they-work-in-ChatGPT), [5](https://massedcompute.com/faq-answers/?question=What+are+the+key+differences+between+the+architecture+of+transformer-based+and+non-transformer+based+large+language+models%3F).\n",
        "   - **Norm и residual connection**: Стабилизируют обучение [10]().\n",
        "\n",
        "2. **Decoder** (используется в задачах генерации):\n",
        "   - Слой **masked self-attention**: Учитывает только предыдущие токены при генерации [4](https://medium.com/@nimritakoul01/the-transformer-model-all-components-simply-explained-series-38c76ecdeeef).\n",
        "   - Слой **encoder-decoder attention**: Учитывает информацию из encoder-а [9](https://myscale.com/blog/key-components-transformer-models-understanding/).\n",
        "\n",
        "3. **Multi-head attention**:\n",
        "   - Параллельно вычисляет несколько attention-векторов для лучшего захвата контекста [2](https://www.quora.com/What-are-the-key-components-of-the-Transformer-architecture-and-how-do-they-work-in-ChatGPT), [7](https://www.portotheme.com/what-are-the-key-differences-between-transformer-models-and-traditional-neural-networks/).\n",
        "\n",
        "### Пример архитектуры BERT:\n",
        "   - Только encoder (поскольку задача классификации, а не генерации).\n",
        "   - 12 или 24 слоев encoder.\n",
        "   - Использует маскирование для обучения на замаскированных токенах [3](https://www.datacamp.com/tutorial/how-transformers-work).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIT5Usam7K_d"
      },
      "source": [
        "## Задача классификации тональности текста\n",
        "**Цель**: Определить, является ли отзыв на фильм положительным или отрицательным.\n",
        "**Датасет**: IMDb movie reviews (25,000 примеров).\n",
        "\n",
        "### Этапы решения:\n",
        "1. **Подготовка данных**: Разделение на train/val/test.\n",
        "2. **Токенизация**: Преобразование текста в тензоры через BERT-токенизатор.\n",
        "3. **Обучение**: Использование предобученной модели BERT.\n",
        "4. **Оценка**: Вычисление точности и F1-меры.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nICMU7pg7K_d"
      },
      "source": [
        "## Код для решения задачи\n",
        "```python\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Загрузка датасета\n",
        "dataset = load_dataset('imdb')\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# Инициализация токенизатора и модели\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2  # 2 класса: positive/negative\n",
        ")\n",
        "\n",
        "# Функция токенизации\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'],\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=512)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
        "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
        "\n",
        "# Конвертация в DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# Обучение\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Оценка на тесте\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in test_loader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**inputs)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (preds == batch['label'].to(device)).sum().item()\n",
        "            total += len(batch['label'])\n",
        "        print(f'Epoch {epoch}: Accuracy = {correct/total:.4f}')\n",
        "```\n",
        "\n",
        "### Пояснение кода:\n",
        "1. **Токенизация**:\n",
        "   - `tokenizer` преобразует текст в числовые токены и добавляет специальные токены `[CLS]` и `[SEP]` [3](https://www.datacamp.com/tutorial/how-transformers-work).\n",
        "2. **Модель BERT**:\n",
        "   - `BertForSequenceClassification` добавляет классификатор на основе выхода encoder-а [3](https://www.datacamp.com/tutorial/how-transformers-work).\n",
        "3. **Обучение**:\n",
        "   - `loss.backward()` вычисляет градиенты через attention-механизм и полносвязные слои [10](https://lih-verma.medium.com/components-of-transformer-architecture-748f74a1a40d).\n",
        "4. **Оценка**:\n",
        "   - `torch.argmax` выбирает класс с максимальной вероятностью из выхода softmax [3](https://www.datacamp.com/tutorial/how-transformers-work).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ViMoVi7K_d"
      },
      "source": [
        "## Пример работы кода\n",
        "Пусть у нас есть отзыв:\n",
        "```\n",
        "text = 'This movie was incredibly boring and predictable.'\n",
        "```\n",
        "1. **Токенизация**:\n",
        "   - Текст преобразуется в: `[101, 1244, 2113, 1107, 1226, 1110, 1190, 102]`.\n",
        "2. **Обработка модели**:\n",
        "   - **Self-attention**: Каждый токен оценивается относительно других.\n",
        "   - **Feed-forward**: Извлекаются признаки, указывающие на негативный тон.\n",
        "3. **Выход**: Модель вернет `label=1` (отрицательный).\n",
        "\n",
        "### Ключевые шаги в коде:\n",
        "1. **masked self-attention**: Не используется, так как задача классификации [4](https://medium.com/@nimritakoul01/the-transformer-model-all-components-simply-explained-series-38c76ecdeeef).\n",
        "2. **Positional encoding**: Добавляется автоматически в BERT [8](https://t.me/s/rybolos_channel?before=569), [9](https://myscale.com/blog/key-components-transformer-models-understanding/).\n",
        "3. **Классификация**: Выход encoder-а подается на полносвязный слой для предсказания [3](https://www.datacamp.com/tutorial/how-transformers-work).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzBQeVor7K_d"
      },
      "source": [
        "## Задача для закрепления материала\n",
        "**Задача**: Классификация новостей по темам (например, спорт, политика, технологии).\n",
        "\n",
        "### Требования:\n",
        "1. Используйте датасет **AG News** (4 класса, 120k примеров).\n",
        "2. Примените модель **DistilBERT** (упрощенный вариант BERT).\n",
        "3. Оцените модель через **F1-меру**.\n",
        "\n",
        "### Подсказки:\n",
        "1. Загрузите датасет через `load_dataset('ag_news')`.\n",
        "2. Используйте `DistilBertTokenizer` и `DistilBertForSequenceClassification`.\n",
        "3. Измените `num_labels=4`.\n",
        "\n",
        "### Пример кода:\n",
        "```python\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=4  # 4 класса\n",
        ")\n",
        "```\n",
        "\n",
        "### Ожидаемый результат:\n",
        "Точность на тесте должна быть >85%.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y-oJhY97K_d"
      },
      "source": [
        "# **Метод Hybrid Models: подробное объяснение**\n",
        "## Основы\n",
        "1. **Что такое Hybrid Models**:\n",
        "   - Комбинация **Embedding** + **CNN/RNN/Transformers** для объединения преимуществ разных подходов [7](https://www.sciencedirect.com/science/article/pii/S277267112400216X), [9]().\n",
        "   - Например: Embedding для векторизации → CNN для локальных паттернов → LSTM для контекста [6](https://www.sciencedirect.com/science/article/abs/pii/S0378779622001389).\n",
        "\n",
        "2. **Преимущества**:\n",
        "   - **Локальные признаки (CNN)**: Извлекают ключевые фразы и нграммы [5](https://machinelearningmastery.com/cnn-long-short-term-memory-networks/).\n",
        "   - **Контекст (RNN/Transformers)**: Учитывают связь между словами в предложении [13](https://huggingface.co/blog/rwkv).\n",
        "   - **Глобальные признаки (Embedding)**: Предоставляют базовые представления слов [1](https://www.cloudflare.com/ru-ru/learning/ai/what-are-embeddings/), [2](https://habr.com/ru/companies/otus/articles/787116/).\n",
        "\n",
        "3. **Примеры архитектур**:\n",
        "   - **CNN + LSTM**: CNN извлекает фичи, LSTM анализирует их последовательность [5](https://machinelearningmastery.com/cnn-long-short-term-memory-networks/).\n",
        "   - **Embedding + Transformer**: Базовые векторы + глобальный контекст через attention [9](https://arxiv.org/abs/2312.11825), [14](https://medium.com/@data-overload/unlocking-sequential-understanding-recurrent-neural-networks-rnns-and-transformers-dcf83fbe0c9).\n",
        "   - **Hybrid BERT + RNN**: Усилить глобальный контекст предобученной модели [4](https://medium.com/@pandeyarpit88/building-a-hybrid-architecture-harnessing-the-power-of-microservices-and-monoliths-80f79b310d75).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlajQp2o7K_d"
      },
      "source": [
        "## Общий синтаксис и архитектура\n",
        "### Пример: Hybrid model для классификации (Embedding + CNN + LSTM):\n",
        "1. **Embedding layer**:\n",
        "   - Преобразует слова в векторы фиксированной длины [2](https://habr.com/ru/companies/otus/articles/787116/).\n",
        "2. **CNN layers**:\n",
        "   - **Conv1D**: Извлекает локальные признаки (например, эмоциональные выражения) [8](https://medium.com/@mijanr/different-ways-to-combine-cnn-and-lstm-networks-for-time-series-classification-tasks-b03fc37e91b6).\n",
        "   - **MaxPooling1D**: Снижает размерность, сохраняя ключевые фичи [8](https://medium.com/@mijanr/different-ways-to-combine-cnn-and-lstm-networks-for-time-series-classification-tasks-b03fc37e91b6).\n",
        "3. **RNN/LSTM layer**:\n",
        "   - Обрабатывает последовательность фич из CNN для уловия контекста [3](https://link.springer.com/chapter/10.1007/978-3-031-19608-9_4).\n",
        "4. **Dense layers**:\n",
        "   - Полносвязные слои для финальной классификации [2](https://habr.com/ru/companies/otus/articles/787116/).\n",
        "5. **Регуляризация**:\n",
        "   - Dropout и BatchNormalization для предотвращения переобучения [3](https://link.springer.com/chapter/10.1007/978-3-031-19608-9_4).\n",
        "\n",
        "### Пример архитектуры:\n",
        "```python\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "```\n",
        "### Пояснение:\n",
        "1. **Embedding**: Векторы слов размером 100 [2](https://habr.com/ru/companies/otus/articles/787116/).\n",
        "2. **Conv1D**: Фильтры размером 5 для локальных паттернов [10](https://link.springer.com/article/10.1007/s11063-024-11687-w).\n",
        "3. **LSTM**: Обрабатывает 64 нейрона для контекста [10](https://link.springer.com/article/10.1007/s11063-024-11687-w).\n",
        "4. **Dropout**: 50% нейронов отключаются для регуляризации [11](https://www.researchgate.net/figure/Hybrid-CNN-LSTM-model_fig5_355373442).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27CeT7T-7K_h"
      },
      "source": [
        "## Задача классификации тональности текста\n",
        "**Цель**: Определить, является ли отзыв на фильм положительным или отрицательным.\n",
        "**Датасет**: IMDb movie reviews (25,000 примеров).\n",
        "\n",
        "### Этапы решения:\n",
        "1. **Подготовка данных**: Токенизация, разбиение на train/val/test.\n",
        "2. **Построение Hybrid model**:\n",
        "   - Embedding → CNN → LSTM → Dense.\n",
        "3. **Обучение**: Использование категориальной кросс-энтропии.\n",
        "4. **Оценка**: Точность и матрица ошибок.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s9FoyZD7K_h"
      },
      "source": [
        "## Код для решения задачи\n",
        "```python\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "\n",
        "# Загрузка данных\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
        "max_length = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_length)\n",
        "X_test = pad_sequences(X_test, maxlen=max_length)\n",
        "\n",
        "# Построение Hybrid model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=100, input_length=max_length),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPooling1D(4),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # 2 класса → sigmoid\n",
        "])\n",
        "\n",
        "# Компиляция и обучение\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train,\n",
        "                   epochs=5,\n",
        "                   validation_split=0.2)\n",
        "\n",
        "# Оценка\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {test_acc:.4f}')\n",
        "```\n",
        "\n",
        "### Пояснение кода:\n",
        "1. **Embedding**: 5000 слов → векторы размера 100 [2](https://habr.com/ru/companies/otus/articles/787116/).\n",
        "2. **Conv1D**: Фильтры 5x128 ищут локальные паттерны [12](https://www.nature.com/articles/s41598-024-73452-2).\n",
        "3. **MaxPooling**: Снижает размерность до 124 (500-5+1)/4 ≈ 124 [12](https://www.nature.com/articles/s41598-024-73452-2).\n",
        "4. **LSTM**: Обрабатывает 64 нейрона для глобального контекста [12](https://www.nature.com/articles/s41598-024-73452-2).\n",
        "5. **Dropout**: 50% отключения нейронов для регуляризации [12](https://www.nature.com/articles/s41598-024-73452-2).\n",
        "6. **Sigmoid**: Для бинарной классификации [2](https://habr.com/ru/companies/otus/articles/787116/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrqYRb8a7K_h"
      },
      "source": [
        "## Пример работы кода\n",
        "Пусть у нас есть отзыв:\n",
        "```\n",
        "text = 'This movie was incredibly boring and predictable.'\n",
        "```\n",
        "1. **Токенизация**:\n",
        "   - Преобразуется в последовательность индексов (например, [2, 56, 1000, ...]).\n",
        "2. **Embedding**:\n",
        "   - Каждый индекс → вектор длиной 100.\n",
        "3. **CNN**:\n",
        "   - Фильтры анализируют окна из 5 слов, выделяя признаки вроде 'boring' и 'predictable'.\n",
        "4. **LSTM**:\n",
        "   - Учитывает связь между фичами, например, 'incredibly' усиливает негатив [12](https://www.nature.com/articles/s41598-024-73452-2).\n",
        "5. **Выход**: Вероятность 0.9 для класса 'negative'.\n",
        "\n",
        "### Ключевые шаги:\n",
        "1. **Conv1D**: Извлекает фичи типа 'negative phrases'.\n",
        "2. **LSTM**: Соединяет локальные фичи в глобальный контекст.\n",
        "3. **Dropout**: Предотвращает переобучение на шумовых фичах.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbsAPRg77K_h"
      },
      "source": [
        "## Задача для закрепления материала\n",
        "**Задача**: Классификация новостей по категориям (спорт, политика, технологии).\n",
        "\n",
        "### Требования:\n",
        "1. Используйте датасет **Reuters** (10 классов, ~10k примеров).\n",
        "2. Постройте Hybrid model с **CNN + GRU**.\n",
        "3. Добавьте **BatchNormalization** после каждого слоя.\n",
        "\n",
        "### Подсказки:\n",
        "1. Загрузите данные через `keras.datasets.reuters`.\n",
        "2. Архитектура:\n",
        "   ```python\n",
        "   model = Sequential([\n",
        "       Embedding(...),\n",
        "       Conv1D(...),\n",
        "       GRU(32),\n",
        "       Dense(...)\n",
        "   ])\n",
        "   ```\n",
        "3. Оптимизатор: Adam, learning_rate=0.001.\n",
        "\n",
        "### Ожидаемый результат:\n",
        "Точность на тесте > 80%.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}