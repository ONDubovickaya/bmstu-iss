{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5yXtAXtC5Dd"
      },
      "source": [
        "---\n",
        "## **Лекция: Прореживание (прунинг) нейронных сетей**\n",
        "### **Цель:** Изучить методы оптимизации нейронных сетей через удаление избыточных параметров.\n",
        "\n",
        "### **Теоретическая часть**\n",
        "#### **1. Основные концепции и терминология**\n",
        "**Переобучение (Overfitting)** — явление, когда модель слишком точно подстраивается под обучающие данные и теряет способность к обобщению. Регуляризация включает методы борьбы с переобучением, включая L1/L2-регуляризацию и прореживание.\n",
        "\n",
        "**Прунинг (Pruning)** — процесс удаления менее значимых элементов нейросети (весов, нейронов, слоев) для уменьшения размера модели и ускорения вычислений. Эмпирически доказано: нейросети содержат до 90% избыточных параметров.\n",
        "\n",
        "#### **2. Виды прунинга**\n",
        "**Неструктурированный прунинг** — удаление отдельных весов. Пример: magnitude-based pruning (удаление весов с наименьшими абсолютными значениями).\n",
        "\n",
        "**Структурированный прунинг** — удаление целых блоков сети (нейронов, каналов свёрток). Эффективнее для аппаратной реализации.\n",
        "\n",
        "**По времени применения:**\n",
        "- До обучения (на случайной инициализации)\n",
        "- Во время обучения (с L1-регуляризацией)\n",
        "- После обучения (оптимизация готовой модели)\n",
        "\n",
        "#### **3. Критерии значимости параметров**\n",
        "- **Magnitude-based:** Веса с малыми абсолютными значениями считаются неважными.\n",
        "- **Gradient-based:** Анализ влияния весов на градиент функции потерь.\n",
        "- **Hessian-based:** Учёт кривизны функции потерь (OBD/OBS-методы).\n",
        "\n",
        "#### **4. Связь с другими методами оптимизации**\n",
        "- **Квантизация (Quantization):** Сокращение разрядности весов (например, float32 → int8).\n",
        "- **Дистилляция знаний (Knowledge Distillation):** Передача знаний от большой модели к меньшей.\n",
        "- **Низкоранговое разложение (Low-Rank):** Аппроксимация матриц весов.\n",
        "\n",
        "#### **5. Историческая эволюция методов прунинга**\n",
        "**1989-1993: Зарождение концепций**  \n",
        "- Работа <NAME> *«Optimal Brain Damage»* ввела понятие значимости весов через анализ гессиана.  \n",
        "- Эксперименты на мелких сетях (N<1000 параметров) показали возможность 10-кратного сжатия без потери точности.\n",
        "\n",
        "**2015-2018: Эра глубокого обучения**  \n",
        "- Magnitude pruning стал стандартом для ResNet (Han et al., 2015).  \n",
        "- Структурированный прунинг для свёрточных сетей: удаление фильтров по L1-норме выходных каналов (Li et al., 2016).\n",
        "\n",
        "**2020-2024: Нейросинтез и автоматизация**  \n",
        "- Методы вроде *Neural Architecture Search* (NAS) интегрируют прунинг в процесс проектирования архитектур.  \n",
        "- Техники динамического обновления масок (RigL) достигают 85% сжатия на Transformers.\n",
        "\n",
        "#### **6. Математические основы прунинга**\n",
        "**Формализация задачи:**  \n",
        "Для модели с весами $W$ найти бинарную маску $M$, минимизирующую:  \n",
        "$$\n",
        "\\mathcal{L}_{\\text{prune}} = \\mathcal{L}(W \\odot M) + \\lambda \\|M\\|_0 \\to \\min,\n",
        "$$  \n",
        "где $\\|M\\|_0$ — количество активных параметров.\n",
        "\n",
        "**Типы регуляризации:**  \n",
        "- **L0-аппроксимация:** Замена недифференцируемой L0-нормы на гладкую функцию:  \n",
        "$$\n",
        "\\|M\\|_0 \\approx \\sum \\sigma(\\beta w_i), \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
        "$$\n",
        "- **Стохастическое прореживание:** Моделирование маски как бернуллиевских случайных величин.\n",
        "\n",
        "#### **7. Продвинутые алгоритмы прунинга**\n",
        "**Hessian-ориентированные методы:**  \n",
        "- **OBS (Optimal Brain Surgeon):** Учёт недиагональных элементов гессиана:  \n",
        "$$\n",
        "\\Delta \\mathcal{L} = \\frac{1}{2} w_i^2 [H^{-1}]_{ii}^{-1}.\n",
        "$$  \n",
        "- **Критерий Тейлора:** Важность веса $w_i$ оценивается через:  \n",
        "$$\n",
        "I_i = |w_i \\cdot \\nabla_{w_i} \\mathcal{L}|.\n",
        "$$\n",
        "\n",
        "**SNIP (Single-shot Network Pruning):**  \n",
        "Ранжирование весов по чувствительности до обучения:  \n",
        "$$\n",
        "S_i = \\left| w_i \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w_i} \\right|.\n",
        "$$\n",
        "\n",
        "#### **8. Практические аспекты реализации**\n",
        "**Обработка различных слоёв:**  \n",
        "- **Conv-слои:** Удаление фильтров с нормой ниже порога $\\theta = \\mu - 2\\sigma$.  \n",
        "- **BatchNorm:** Учет параметров масштаба/сдвига при структурированном прунинге.\n",
        "- **Skip-connections:** Запрет обрезки в residual-блоках для сохранения градиентов.\n",
        "\n",
        "**Гиперпараметры:**  \n",
        "- Итеративное прореживание с `final_sparsity=0.8`, `frequency=100` шагов.  \n",
        "- Совместная оптимизация с learning rate decay (коэф. $\\eta(t) = \\eta_0 e^{-t/T}$).\n",
        "\n",
        "#### **9. Анализ эффективности**\n",
        "**Метрики:**  \n",
        "- *Compression Ratio (CR):* $CR = \\frac{\\text{Исх. параметры}}{\\text{Прунинг. параметры}}$  \n",
        "- *Accuracy Drop (AD):* $AD = \\text{Acc}_{\\text{base}} - \\text{Acc}_{\\text{pruned}}$  \n",
        "- *Теоретическое ускорение:* Рассчитывается через FLOPs reduction.\n",
        "\n",
        "**Бенчмарки (ImageNet):**  \n",
        "| Метод          | ResNet-50 (AD) | ViT-Base (AD) | CR  |\n",
        "|----------------|----------------|---------------|-----|\n",
        "| Magnitude      | 1.2%           | 3.8%          | 10x |\n",
        "| SNIP           | 0.7%           | 2.1%          | 7x  |\n",
        "| Variational    | 0.9%           | 1.9%          | 12x |\n",
        "\n",
        "#### **10. Современные исследования и вызовы**\n",
        "**Тенденции 2024:**  \n",
        "- **Прунинг без данных:** Алгоритмы вроде *Data-Free Pruning* (DFP) используют синтетические данные.  \n",
        "- **Кросс-модальное прореживание:** Единая маска для мультимодальных моделей (текст+изображение).\n",
        "\n",
        "**Проблемы:**  \n",
        "- Деградация качества при CR > 20x из-за нарушения manifold-структуры данных.  \n",
        "- Неадаптивность масок к доменным сдвигам.\n",
        "\n",
        "#### **11. Интеграция с другими методами**\n",
        "**Квантование + Прунинг:**  \n",
        "- 8-битное квантование после прунинга даёт дополнительное 4x сжатие (Google's QAT).\n",
        "\n",
        "**Дистилляция:**  \n",
        "- Учитель с 100M параметров → студент 20M + прунинг до 5M (DistilBERT).\n",
        "\n",
        "### **Примеры кода (дополнение)**\n",
        "**Iterative Pruning в PyTorch:**\n",
        "```python\n",
        "for epoch in range(10):\n",
        "    # Обучение\n",
        "    train_model(model, loader)\n",
        "    \n",
        "    # Прунинг 10% весов каждые 2 эпохи\n",
        "    if epoch % 2 == 0:\n",
        "        prune.global_unstructured(\n",
        "            parameters=model.parameters(),\n",
        "            pruning_method=prune.L1Unstructured,\n",
        "            amount=0.1\n",
        "        )\n",
        "```\n",
        "\n",
        "**Structured Pruning для Conv-слоёв:**\n",
        "```python\n",
        "# Фильтры с наименьшей L2-нормой\n",
        "norms = torch.norm(conv.weight.data, p=2, dim=(1,2,3))\n",
        "thresh = torch.kthvalue(norms, int(0.3 * len(norms))).values\n",
        "mask = norms > thresh\n",
        "conv.weight.data = conv.weight.data[mask]\n",
        "```\n",
        "---\n",
        "### **Практическая часть**\n",
        "#### **Пример 1: Magnitude Pruning в PyTorch**\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Шаг 1: Создаём простую модель\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(784, 256),  # Полносвязный слой\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 10)\n",
        ")\n",
        "\n",
        "# Шаг 2: Применяем прунинг к первому слою (удаляем 30% весов)\n",
        "prune.l1_unstructured(module=model[0], name='weight', amount=0.3)\n",
        "\n",
        "# Шаг 3: Проверяем разреженность\n",
        "sparsity = 100 * float(torch.sum(model[0].weight == 0) / model[0].weight.nelement())\n",
        "print(f'Разреженность после прунинга: {sparsity:.2f}%')\n",
        "```\n",
        "**Объяснение:**\n",
        "1. `prune.l1_unstructured` удаляет веса с наименьшими L1-нормами.\n",
        "2. Параметр `amount=0.3` задаёт долю удаляемых весов.\n",
        "3. Веса заменяются на нули, но сохраняются в параметре `weight_orig`.\n",
        "\n",
        "#### **Пример 2: Автоматическое прореживание через L1-регуляризацию**\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Создаём модель с L1-регуляризацией\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l1(0.01)),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# Анализ нулевых весов\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "zeros = tf.math.count_nonzero(weights == 0).numpy()\n",
        "print(f'Занулено {zeros} весов из {weights.size}')\n",
        "```\n",
        "**Объяснение:**\n",
        "1. L1-регуляризация добавляет штраф за большие значения весов, автоматически зануляя часть из них.\n",
        "2. Коэффициент `0.01` регулирует силу регуляризации.\n",
        "\n",
        "#### **Пример 3: Структурированный прунинг свёрточных каналов**\n",
        "```python\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "# Настраиваем расписание прунинга\n",
        "pruning_params = {\n",
        "    'pruning_schedule': sparsity.PolynomialDecay(\n",
        "        initial_sparsity=0.3,\n",
        "        final_sparsity=0.7,\n",
        "        begin_step=100,\n",
        "        end_step=1000\n",
        "    )\n",
        "}\n",
        "\n",
        "# Создаём модель с прунингом\n",
        "pruned_model = tf.keras.Sequential([\n",
        "    sparsity.prune_low_magnitude(\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        **pruning_params\n",
        "    ),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "```\n",
        "**Объяснение:**\n",
        "1. `PolynomialDecay` плавно увеличивает степень прореживания от 30% до 70%.\n",
        "2. Прунинг применяется к свёрточным фильтрам: удаляются целые каналы с малыми нормами.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS2BRC8kC5Dg"
      },
      "source": [
        "---\n",
        "**Литература и дополнительные материалы**\n",
        "- [Оптимизация производитесности НС](https://www.iae.nsk.su/images/stories/4_Education/3_DisSovet/230530/2-%D0%A1%D0%92%D0%98%D0%A2%D0%9E%D0%92/dissertation_---%D0%A1%D0%92%D0%98%D0%A2%D0%9E%D0%92.pdf)\n",
        "- [Машинное обучение](https://deepmachinelearning.ru/docs/Neural-networks/Regularization/Network-pruning)\n",
        "- [ServerFlow](https://serverflow.ru/blog/stati/7-sposobov-uskorit-rabotu-iskusstvennogo-intellekta-i-sdelat-ego-bolee-lyegkim/)\n",
        "- [Habr](https://habr.com/ru/articles/811221/)\n",
        "- [Habr](https://habr.com/ru/articles/575520//)\n",
        "- [Habr](https://habr.com/ru/articles/848306/)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}